/*
 * For purecap/CheriABI code:
 *   LD_CHERI_PRELOAD=/usr/libcheri/libcheri_caprevoke.so:/usr/libcheri/libcheri_shimalloc.so
 *
 * For hybrid code:
 *   LD_PRELOAD=/usr/lib/libcheri_caprevoke.so:/usr/lib/libcheri_shimalloc.so
 *
 * Set SHIMALLOC_OPSF to a file containing a series of little-endian 4-byte
 * integers, perhaps as generated by the following Python 3 script.  Each
 * integer represents the number of intercepted operations to count before
 * doing a revocation pass.  If the stream ends before the program,
 * shimalloc will print a warning and then simply pass all subsequent
 * operations through.
 *
 * #!/usr/bin/python3
 * import sys
 * for i in range(1,20000):
 *   sys.stdout.buffer.write(b'\xFF\x00\x00\x00')
 *
 */

/*
 * We can stub out revocation completely, should it be desired, for
 * benchmarking.
 */
#define SHIMALLOC_REVOKE

/*
 * To support a kind of limit study, we can also store back all capabilities
 * we read.  This is likely to produce very bad numbers, but on the off
 * chance they're OK, it'd be good to know.
 */
#undef SHIMALLOC_REVOKE_STOREBACK

/*
 * XXX WIP: we might want to benchmark with malias(), too, and if that ever
 * happens, code marked with this indicates a good start towards being able
 * to do that.
 */
#undef SHIMALLOC_ALIAS_ARENA_PAGES

#include <inttypes.h>
#include <stdarg.h>
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

#include <dlfcn.h>
#include <fcntl.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <sys/mman.h>
#include <sys/user.h>
#include <sys/umtx.h>
#include <machine/atomic.h>
#include <sys/caprevoke.h>

#include <cheri/libcheri_caprevoke_internals.h>

static void __attribute__((constructor)) shimalloc_init(void);
static void __attribute__((destructor))  shimalloc_fini(void);

static struct {
	void *(*real_malloc)(size_t sz);
	void *(*real_calloc)(size_t n, size_t sz);
	void *(*real_realloc)(void *p, size_t sz);
	void (*real_free)(void *p);
	int ops_fd;
	int trace_fd;

	struct umutex mtx;
	uint64_t nops_to_next_revoke;
	uint64_t nops_total;
	uint64_t nscans;
#if defined(SHIMALLOC_REVOKE) && defined(SHIMALLOC_ALIAS_ARENA_PAGES)
	void * alias_arena;
#endif
} shimalloc_state = {
	.ops_fd = -1,
	.trace_fd = -1,
};

static void
shimalloc_next_op()
{
	unsigned char v[4] = { 0 };
	ssize_t n;

	if ((n = read(shimalloc_state.ops_fd, v, sizeof v)) != 4) {
		fprintf(stderr, "shimalloc bad read; aborting (%zd errno=%d)\n", n, errno);
		close(shimalloc_state.ops_fd);
		shimalloc_state.ops_fd = -1;
	} else {
		shimalloc_state.nops_to_next_revoke =
			(((uint64_t)v[0]) << 0) |
			(((uint64_t)v[1]) << 8) |
			(((uint64_t)v[2]) << 16) |
			(((uint64_t)v[3]) << 24);
	}
}

static void
shimalloc_init()
{
	char *ops_file, *trace_file;

	fprintf(stderr, "shimalloc: %" PRIu64 " ops pre-init\n",
		shimalloc_state.nops_total);

	if ((ops_file = getenv("SHIMALLOC_OPSF")) != NULL) {
		shimalloc_state.ops_fd = open(ops_file, O_RDONLY);
		if (shimalloc_state.ops_fd == -1) {
			write(2, "shimalloc: cannot open ops file\n", 32);
		} else {
			shimalloc_next_op();
		}
#if defined(SHIMALLOC_REVOKE) && defined(SHIMALLOC_ALIAS_ARENA_PAGES)
		shimalloc_state.alias_arena = mmap(0,
				SHIMALLOC_ALIAS_ARENA_PAGES * PAGE_SIZE,
				PROT_READ|PROT_WRITE, MAP_ANON, -1, 0);
		if (shimalloc_state.alias_arena == NULL) {
			write(2, "shimalloc: alias arena failed\n", 29);
			close(shimalloc_state.ops_fd);
			shimalloc_state.ops_fd = -1;
		}
#endif
	} else {
		write(2, "shimalloc: no ops file\n", 23);
	}

	if ((trace_file = getenv("SHIMALLOC_TRACEF")) != NULL) {
		shimalloc_state.trace_fd = open(trace_file, O_WRONLY|O_APPEND|O_TRUNC|O_CREAT, 0644);
		if (shimalloc_state.trace_fd == -1) {
			write(2, "shimalloc: can't open trace file\n", 33);
		} else {
			write(2, "shimalloc: tracing\n", 21);
		}
	}
}

static void
shimalloc_fini()
{
	fprintf(stderr, "shimalloc: ops=%" PRIu64 " scans=%" PRIu64 "\n",
		shimalloc_state.nops_total, shimalloc_state.nscans);
	return;
}

#ifdef SHIMALLOC_REVOKE

/* XXX Because we are not actually doing revocation, there is no notion of
 * "the sweeper stack" to be excluded here.  For sweeping, we just perform
 * reads from memory across every possible page.
 */

static void
shimalloc_sweep_cacheline(void * __capability *test, void *_rock)
{
	uint64_t tags = __builtin_cheri_cap_load_tags((__cheri_tocap void * __capability)test);
	volatile void * __capability p;

	(void)_rock;

	for ( ; tags != 0 ; (tags >>= 1), test++ ) {
		if (!(tags & 1))
			continue;

		p = *test;
#ifdef SHIMALLOC_REVOKE_STOREBACK
		*test = p;
#endif
	}

}

#define CAPS_PER_CACHELINE	8	// XXX Assumed

static void
shimalloc_sweep_page(void *_pg, void *_rock)
{
	intcap_t *pg = _pg;
	size_t i;
	for (i = 0; i < PAGE_SIZE / sizeof(intcap_t); i += CAPS_PER_CACHELINE) {
		shimalloc_sweep_cacheline((void * __capability *)&pg[i], _rock);
	}
}

static void
shimalloc_sweep_mincore(char *loc, unsigned char *mc, size_t len, void *_rock)
{
	size_t i;

	for (i = 0; i < len; i++) {
		if (mc[i] & MINCORE_MAYHAVECAP) {
			shimalloc_sweep_page(loc + PAGE_SIZE * i, _rock);
		}
	}

}

static void
shimalloc_sweep_kve(struct kinfo_vmentry *kve, void *_rock)
{
	size_t npages;

	if ((kve->kve_max_protection & (KVME_PROT_WRITECAPS|KVME_PROT_READCAPS))
	    == 0) {
		return;
	}

	npages = kve->kve_end - kve->kve_start;
	npages = (npages + PAGE_SIZE - 1) / PAGE_SIZE;

#ifdef __CHERI_PURE_CAPABILITY__
	libcheri_caprevoke_iterate_mincore(kve->kve_cap, npages,
					   shimalloc_sweep_mincore, _rock);
#else
	libcheri_caprevoke_iterate_mincore((void *)kve->kve_start, npages,
					   shimalloc_sweep_mincore, _rock);
#endif
}
#endif /* SHIMALLOC_REVOKE */


/* Acquire lock, though possibly with a bad thread ID. */
static void
shimalloc_lock()
{
	struct umutex *m = &shimalloc_state.mtx;
	uint32_t owner = m->m_owner;
	if (((owner & UMUTEX_CONTESTED) == 0)
	    && atomic_cmpset_rel_32(&m->m_owner, owner, 1)) {
		;
	} else {
		_umtx_op(&shimalloc_state.mtx, UMTX_OP_MUTEX_LOCK, 0, 0, 0);
	}
}

/* Release lock */
static void
shimalloc_unlock()
{
	struct umutex *m = &shimalloc_state.mtx;
	uint32_t owner = m->m_owner;
	if (((owner & UMUTEX_CONTESTED) == 0)
	    && atomic_cmpset_rel_32(&m->m_owner, owner, UMUTEX_UNOWNED)) {
		return;
	} else {
		_umtx_op(&shimalloc_state.mtx, UMTX_OP_MUTEX_UNLOCK, 0, 0, 0);
	}
}

static void
shimalloc_trace(void *pc, const char *opfmt, ...)
{
	va_list va;
	struct timeval tv;
	int n;
	char opbuf[256];
	char buf[300];

	if (shimalloc_state.trace_fd != -1) {
		va_start(va, opfmt);

		gettimeofday(&tv, NULL);

		vsnprintf(opbuf, sizeof(opbuf), opfmt, va);
		n = snprintf(buf, sizeof(buf), "%" PRIu64 "\t%p\t%s\n",
			1000 * (((uint64_t)tv.tv_sec) * 1000000 + tv.tv_usec),
			pc, opbuf);

			write(shimalloc_state.trace_fd, buf, n);

		va_end(va);
	}
}

static void
shimalloc_op()
{
	shimalloc_lock();

	if (shimalloc_state.ops_fd != -1) {
		if (shimalloc_state.nops_to_next_revoke == 0) {
			shimalloc_next_op();
#ifdef SHIMALLOC_REVOKE
			libcheri_caprevoke_iterate_vmmap(shimalloc_sweep_kve, NULL);
			caprevoke(0, NULL, CAPREVOKE_HOARDERS);
#endif
			shimalloc_state.nscans++;
			shimalloc_trace(NULL, "revoke\t\t");
		} else {
			shimalloc_state.nops_to_next_revoke--;
		}
	}

	shimalloc_state.nops_total++;

	shimalloc_unlock();
}

void *
malloc(size_t sz)
{
	void *res;
	shimalloc_op();
	if (!shimalloc_state.real_malloc)
		shimalloc_state.real_malloc  = dlsym(RTLD_NEXT, "malloc" );
	res = shimalloc_state.real_malloc(sz);
	shimalloc_trace(__builtin_return_address(0), "malloc\t%tx\t%zx\t", (__cheri_addr ptrdiff_t)res, sz);
	return res;
}

void *
calloc(size_t n, size_t sz)
{
	void *res;
	shimalloc_op();
	if (!shimalloc_state.real_calloc)
		shimalloc_state.real_calloc  = dlsym(RTLD_NEXT, "calloc" );
	res = shimalloc_state.real_calloc(n,sz);
	shimalloc_trace(__builtin_return_address(0), "calloc\t%tx\t%zx\t%zx\t", (__cheri_addr ptrdiff_t)res, n, sz);
	return res;
}

void *
realloc(void *p, size_t sz)
{
	void *res;

	shimalloc_op();
	if (!shimalloc_state.real_realloc)
		shimalloc_state.real_realloc = dlsym(RTLD_NEXT, "realloc");
	res = shimalloc_state.real_realloc(p,sz);
	shimalloc_trace(__builtin_return_address(0), "realloc\t%tx\t%tx\t%zd\t", (__cheri_addr ptrdiff_t)res, (__cheri_addr ptrdiff_t)p, sz);
	return res;
}

void
free(void *p)
{
	shimalloc_op();
	if (!shimalloc_state.real_free)
		shimalloc_state.real_free    = dlsym(RTLD_NEXT, "free"   );
	shimalloc_state.real_free(p);
	shimalloc_trace(__builtin_return_address(0), "free\t%tx\t", (__cheri_addr ptrdiff_t)p);
}
